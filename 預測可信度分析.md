# é æ¸¬å¯ä¿¡åº¦åˆ†æ

## ğŸ¯ ä»€éº¼æ˜¯é æ¸¬å¯ä¿¡åº¦ï¼Ÿ

**é æ¸¬å¯ä¿¡åº¦ï¼ˆPrediction Confidence/Uncertaintyï¼‰** æ˜¯æŒ‡æ¨¡å‹å°å…¶é æ¸¬çµæœçš„ç¢ºå®šç¨‹åº¦ã€‚

### ç‚ºä»€éº¼é‡è¦ï¼Ÿ

```
é æ¸¬å€¼: é–‹ç›¤æ¼²å¹… = +8.5%

ä½†æ˜¯ï¼š
- å¯ä¿¡åº¦é«˜ï¼ˆ90%ï¼‰â†’ å¯ä»¥ä¿¡è³´é€™å€‹é æ¸¬
- å¯ä¿¡åº¦ä½ï¼ˆ30%ï¼‰â†’ é€™å€‹é æ¸¬ä¸å¤ªå¯é 
```

**å¯¦éš›æ‡‰ç”¨**ï¼š
- å¯ä¿¡åº¦é«˜çš„é æ¸¬ â†’ å¯ä»¥æ¡å–è¡Œå‹•
- å¯ä¿¡åº¦ä½çš„é æ¸¬ â†’ è¬¹æ…è§€æœ›

---

## ğŸ“Š å›æ­¸æ¨¡å‹çš„å¯ä¿¡åº¦æŒ‡æ¨™

### 1. é æ¸¬å€é–“ï¼ˆPrediction Intervalï¼‰

**å®šç¾©**ï¼šé æ¸¬å€¼çš„å¯èƒ½ç¯„åœ

```
é æ¸¬: é–‹ç›¤æ¼²å¹… = +8.5%
95% é æ¸¬å€é–“: [+3.2%, +13.8%]

è§£è®€:
- æœ‰ 95% çš„æ©Ÿç‡ï¼ŒçœŸå¯¦å€¼æœƒè½åœ¨ +3.2% åˆ° +13.8% ä¹‹é–“
- å€é–“è¶Šçª„ â†’ å¯ä¿¡åº¦è¶Šé«˜
- å€é–“è¶Šå¯¬ â†’ å¯ä¿¡åº¦è¶Šä½
```

**è¨ˆç®—æ–¹æ³•**ï¼š
```python
from sklearn.ensemble import GradientBoostingRegressor
import numpy as np

# ä½¿ç”¨ Quantile Regression ä¼°è¨ˆé æ¸¬å€é–“
model_lower = GradientBoostingRegressor(loss='quantile', alpha=0.025)  # 2.5%
model_upper = GradientBoostingRegressor(loss='quantile', alpha=0.975)  # 97.5%

model_lower.fit(X_train, y_train)
model_upper.fit(X_train, y_train)

# é æ¸¬
y_pred = model.predict(X_test)
y_lower = model_lower.predict(X_test)
y_upper = model_upper.predict(X_test)

# é æ¸¬å€é–“
prediction_interval = y_upper - y_lower
```

### 2. æ¨™æº–èª¤å·®ï¼ˆStandard Errorï¼‰

**å®šç¾©**ï¼šé æ¸¬çš„å¹³å‡èª¤å·®

```python
# è¨ˆç®—æ¨™æº–èª¤å·®
from sklearn.metrics import mean_squared_error

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
std_error = np.sqrt(mse)

print(f"æ¨™æº–èª¤å·®: {std_error:.2f}%")
# ä¾‹å¦‚: æ¨™æº–èª¤å·®: 4.75%

# å¯ä¿¡åº¦è§£è®€
# é æ¸¬å€¼ Â± 2Ã—æ¨™æº–èª¤å·® ç´„æ¶µè“‹ 95% çš„æƒ…æ³
confidence_interval = 2 * std_error
print(f"95% ä¿¡è³´å€é–“: Â±{confidence_interval:.2f}%")
```

### 3. æ¨¡å‹ä¸ç¢ºå®šæ€§ï¼ˆModel Uncertaintyï¼‰

**æ–¹æ³• 1: ä½¿ç”¨å¤šå€‹æ¨¡å‹ï¼ˆEnsembleï¼‰**

```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
import xgboost as xgb

# è¨“ç·´å¤šå€‹ä¸åŒçš„æ¨¡å‹
models = {
    'XGBoost': xgb.XGBRegressor(),
    'RandomForest': RandomForestRegressor(),
    'GradientBoosting': GradientBoostingRegressor(),
    'Ridge': Ridge()
}

predictions = []
for name, model in models.items():
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    predictions.append(pred)

# è¨ˆç®—é æ¸¬çš„å¹³å‡å€¼å’Œæ¨™æº–å·®
predictions = np.array(predictions)
mean_pred = predictions.mean(axis=0)
std_pred = predictions.std(axis=0)

# æ¨™æº–å·®è¶Šå¤§ â†’ æ¨¡å‹ä¹‹é–“åˆ†æ­§è¶Šå¤§ â†’ å¯ä¿¡åº¦è¶Šä½
print(f"é æ¸¬å€¼: {mean_pred[0]:.2f}%")
print(f"æ¨¡å‹åˆ†æ­§åº¦: {std_pred[0]:.2f}%")
```

**æ–¹æ³• 2: Bootstrap é‡æ¡æ¨£**

```python
from sklearn.utils import resample

# è¨“ç·´å¤šå€‹æ¨¡å‹ï¼ˆä½¿ç”¨ä¸åŒçš„è¨“ç·´å­é›†ï¼‰
n_iterations = 100
predictions = []

for i in range(n_iterations):
    # é‡æ¡æ¨£è¨“ç·´è³‡æ–™
    X_boot, y_boot = resample(X_train, y_train, random_state=i)
    
    # è¨“ç·´æ¨¡å‹
    model = xgb.XGBRegressor(random_state=i)
    model.fit(X_boot, y_boot)
    
    # é æ¸¬
    pred = model.predict(X_test)
    predictions.append(pred)

# è¨ˆç®—çµ±è¨ˆé‡
predictions = np.array(predictions)
mean_pred = predictions.mean(axis=0)
std_pred = predictions.std(axis=0)
lower_bound = np.percentile(predictions, 2.5, axis=0)
upper_bound = np.percentile(predictions, 97.5, axis=0)

print(f"é æ¸¬å€¼: {mean_pred[0]:.2f}%")
print(f"95% é æ¸¬å€é–“: [{lower_bound[0]:.2f}%, {upper_bound[0]:.2f}%]")
print(f"å€é–“å¯¬åº¦: {upper_bound[0] - lower_bound[0]:.2f}%")
```

### 4. ç‰¹å¾µç›¸ä¼¼åº¦ï¼ˆFeature Similarityï¼‰

**æ¦‚å¿µ**ï¼šæ–°è³‡æ–™èˆ‡è¨“ç·´è³‡æ–™çš„ç›¸ä¼¼ç¨‹åº¦

```python
from sklearn.neighbors import NearestNeighbors

# æ‰¾å‡ºè¨“ç·´é›†ä¸­æœ€ç›¸ä¼¼çš„æ¨£æœ¬
nn = NearestNeighbors(n_neighbors=5)
nn.fit(X_train)

# è¨ˆç®—æ–°æ¨£æœ¬åˆ°æœ€è¿‘é„°å±…çš„è·é›¢
distances, indices = nn.kneighbors(X_test)

# å¹³å‡è·é›¢è¶Šå° â†’ è¶Šç›¸ä¼¼ â†’ å¯ä¿¡åº¦è¶Šé«˜
avg_distance = distances.mean(axis=1)

# æ¨™æº–åŒ–è·é›¢ï¼ˆ0-1ï¼‰
max_distance = distances.max()
similarity_score = 1 - (avg_distance / max_distance)

print(f"ç›¸ä¼¼åº¦åˆ†æ•¸: {similarity_score[0]:.2f}")
# 0.9 â†’ éå¸¸ç›¸ä¼¼ï¼Œå¯ä¿¡åº¦é«˜
# 0.3 â†’ ä¸å¤ªç›¸ä¼¼ï¼Œå¯ä¿¡åº¦ä½
```

---

## ğŸ¯ XGBoost çš„å¯ä¿¡åº¦ä¼°è¨ˆ

### æ–¹æ³• 1: ä½¿ç”¨æ¨¹çš„é æ¸¬åˆ†å¸ƒ

```python
import xgboost as xgb

# è¨“ç·´æ¨¡å‹
model = xgb.XGBRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ç²å–æ¯æ£µæ¨¹çš„é æ¸¬
booster = model.get_booster()
# æ³¨æ„ï¼šXGBoost ä¸ç›´æ¥æä¾›æ¯æ£µæ¨¹çš„é æ¸¬ï¼Œéœ€è¦è‡ªå·±å¯¦ä½œ

# æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ predict çš„ ntree_limit åƒæ•¸
predictions = []
for n_trees in range(10, 101, 10):
    pred = model.predict(X_test, ntree_limit=n_trees)
    predictions.append(pred)

predictions = np.array(predictions)
std_pred = predictions.std(axis=0)

print(f"é æ¸¬ä¸ç¢ºå®šæ€§: {std_pred[0]:.2f}%")
```

### æ–¹æ³• 2: ä½¿ç”¨ Quantile Regression

```python
# è¨“ç·´ä¸‰å€‹æ¨¡å‹ï¼šä¸‹ç•Œã€ä¸­ä½æ•¸ã€ä¸Šç•Œ
model_lower = xgb.XGBRegressor(
    objective='reg:quantileerror',
    quantile_alpha=0.05,  # 5% åˆ†ä½æ•¸
    n_estimators=100
)

model_median = xgb.XGBRegressor(
    objective='reg:quantileerror',
    quantile_alpha=0.5,   # 50% åˆ†ä½æ•¸ï¼ˆä¸­ä½æ•¸ï¼‰
    n_estimators=100
)

model_upper = xgb.XGBRegressor(
    objective='reg:quantileerror',
    quantile_alpha=0.95,  # 95% åˆ†ä½æ•¸
    n_estimators=100
)

# è¨“ç·´
model_lower.fit(X_train, y_train)
model_median.fit(X_train, y_train)
model_upper.fit(X_train, y_train)

# é æ¸¬
y_lower = model_lower.predict(X_test)
y_median = model_median.predict(X_test)
y_upper = model_upper.predict(X_test)

# 90% é æ¸¬å€é–“
print(f"é æ¸¬å€¼: {y_median[0]:.2f}%")
print(f"90% é æ¸¬å€é–“: [{y_lower[0]:.2f}%, {y_upper[0]:.2f}%]")
print(f"å€é–“å¯¬åº¦: {y_upper[0] - y_lower[0]:.2f}%")
```

---

## ğŸ’¡ å¯¦ç”¨çš„å¯ä¿¡åº¦æŒ‡æ¨™

### ç¶œåˆå¯ä¿¡åº¦åˆ†æ•¸

```python
def calculate_confidence_score(
    prediction,
    prediction_interval_width,
    similarity_score,
    model_std
):
    """
    è¨ˆç®—ç¶œåˆå¯ä¿¡åº¦åˆ†æ•¸ï¼ˆ0-1ï¼‰
    
    åƒæ•¸:
    - prediction: é æ¸¬å€¼
    - prediction_interval_width: é æ¸¬å€é–“å¯¬åº¦
    - similarity_score: èˆ‡è¨“ç·´è³‡æ–™çš„ç›¸ä¼¼åº¦
    - model_std: æ¨¡å‹é æ¸¬çš„æ¨™æº–å·®
    """
    
    # 1. é æ¸¬å€é–“åˆ†æ•¸ï¼ˆå€é–“è¶Šçª„è¶Šå¥½ï¼‰
    # å‡è¨­å€é–“å¯¬åº¦ < 5% ç‚ºé«˜å¯ä¿¡åº¦
    interval_score = max(0, 1 - prediction_interval_width / 10)
    
    # 2. ç›¸ä¼¼åº¦åˆ†æ•¸ï¼ˆå·²ç¶“æ˜¯ 0-1ï¼‰
    # similarity_score å·²ç¶“æ¨™æº–åŒ–
    
    # 3. æ¨¡å‹ä¸€è‡´æ€§åˆ†æ•¸ï¼ˆæ¨™æº–å·®è¶Šå°è¶Šå¥½ï¼‰
    # å‡è¨­æ¨™æº–å·® < 2% ç‚ºé«˜å¯ä¿¡åº¦
    consistency_score = max(0, 1 - model_std / 4)
    
    # ç¶œåˆåˆ†æ•¸ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
    confidence = (
        0.4 * interval_score +
        0.3 * similarity_score +
        0.3 * consistency_score
    )
    
    return confidence

# ä½¿ç”¨ç¯„ä¾‹
confidence = calculate_confidence_score(
    prediction=8.5,
    prediction_interval_width=6.2,  # [+5.4%, +11.6%]
    similarity_score=0.75,
    model_std=1.8
)

print(f"å¯ä¿¡åº¦åˆ†æ•¸: {confidence:.2f}")
# 0.8-1.0: é«˜å¯ä¿¡åº¦
# 0.5-0.8: ä¸­ç­‰å¯ä¿¡åº¦
# 0.0-0.5: ä½å¯ä¿¡åº¦
```

---

## ğŸ”§ å¯¦ä½œï¼šç‚ºä½ çš„å°ˆæ¡ˆå¢åŠ å¯ä¿¡åº¦

### æ­¥é©Ÿ 1: ä¿®æ”¹é æ¸¬ç¨‹å¼

```python
def predict_with_confidence(model, X_new, X_train, y_train):
    """
    é æ¸¬ä¸¦è¨ˆç®—å¯ä¿¡åº¦
    
    è¿”å›:
    - prediction: é æ¸¬å€¼
    - confidence: å¯ä¿¡åº¦åˆ†æ•¸ï¼ˆ0-1ï¼‰
    - interval: é æ¸¬å€é–“ [lower, upper]
    """
    
    # 1. åŸºæœ¬é æ¸¬
    prediction = model.predict(X_new)[0]
    
    # 2. ä½¿ç”¨ Bootstrap ä¼°è¨ˆé æ¸¬å€é–“
    n_iterations = 50
    predictions = []
    
    for i in range(n_iterations):
        # é‡æ¡æ¨£
        indices = np.random.choice(len(X_train), len(X_train), replace=True)
        X_boot = X_train[indices]
        y_boot = y_train[indices]
        
        # è¨“ç·´
        boot_model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=i
        )
        boot_model.fit(X_boot, y_boot)
        
        # é æ¸¬
        pred = boot_model.predict(X_new)[0]
        predictions.append(pred)
    
    predictions = np.array(predictions)
    
    # 3. è¨ˆç®—çµ±è¨ˆé‡
    mean_pred = predictions.mean()
    std_pred = predictions.std()
    lower = np.percentile(predictions, 2.5)
    upper = np.percentile(predictions, 97.5)
    interval_width = upper - lower
    
    # 4. è¨ˆç®—ç›¸ä¼¼åº¦
    from sklearn.neighbors import NearestNeighbors
    nn = NearestNeighbors(n_neighbors=5)
    nn.fit(X_train)
    distances, _ = nn.kneighbors(X_new)
    avg_distance = distances.mean()
    max_distance = np.linalg.norm(X_train.max(axis=0) - X_train.min(axis=0))
    similarity_score = 1 - (avg_distance / max_distance)
    
    # 5. è¨ˆç®—å¯ä¿¡åº¦
    confidence = calculate_confidence_score(
        prediction=mean_pred,
        prediction_interval_width=interval_width,
        similarity_score=similarity_score,
        model_std=std_pred
    )
    
    return {
        'prediction': prediction,
        'mean_prediction': mean_pred,
        'confidence': confidence,
        'interval': (lower, upper),
        'interval_width': interval_width,
        'std': std_pred,
        'similarity': similarity_score
    }
```

### æ­¥é©Ÿ 2: ä½¿ç”¨ç¯„ä¾‹

```python
# é æ¸¬
result = predict_with_confidence(model, X_new, X_train, y_train)

print(f"é æ¸¬å€¼: {result['prediction']:.2f}%")
print(f"å¯ä¿¡åº¦: {result['confidence']:.2f}")
print(f"95% é æ¸¬å€é–“: [{result['interval'][0]:.2f}%, {result['interval'][1]:.2f}%]")
print(f"ç›¸ä¼¼åº¦: {result['similarity']:.2f}")

# æ ¹æ“šå¯ä¿¡åº¦æ±ºç­–
if result['confidence'] > 0.7:
    print("âœ… é«˜å¯ä¿¡åº¦ï¼Œå¯ä»¥æ¡å–è¡Œå‹•")
elif result['confidence'] > 0.4:
    print("âš ï¸ ä¸­ç­‰å¯ä¿¡åº¦ï¼Œè¬¹æ…è©•ä¼°")
else:
    print("âŒ ä½å¯ä¿¡åº¦ï¼Œå»ºè­°è§€æœ›")
```

---

## ğŸ“Š å¯ä¿¡åº¦çš„è¦–è¦ºåŒ–

```python
import matplotlib.pyplot as plt

def plot_prediction_with_confidence(results):
    """è¦–è¦ºåŒ–é æ¸¬çµæœå’Œå¯ä¿¡åº¦"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # åœ–1: é æ¸¬å€é–“
    stocks = [r['stock'] for r in results]
    predictions = [r['prediction'] for r in results]
    lowers = [r['interval'][0] for r in results]
    uppers = [r['interval'][1] for r in results]
    confidences = [r['confidence'] for r in results]
    
    # é¡è‰²æ ¹æ“šå¯ä¿¡åº¦
    colors = ['green' if c > 0.7 else 'orange' if c > 0.4 else 'red' 
              for c in confidences]
    
    ax1.scatter(range(len(stocks)), predictions, c=colors, s=100, zorder=3)
    for i in range(len(stocks)):
        ax1.plot([i, i], [lowers[i], uppers[i]], c=colors[i], linewidth=2)
    
    ax1.set_xticks(range(len(stocks)))
    ax1.set_xticklabels(stocks, rotation=45)
    ax1.set_ylabel('é æ¸¬æ¼²å¹… (%)')
    ax1.set_title('é æ¸¬å€¼èˆ‡é æ¸¬å€é–“')
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    
    # åœ–2: å¯ä¿¡åº¦åˆ†æ•¸
    ax2.barh(range(len(stocks)), confidences, color=colors)
    ax2.set_yticks(range(len(stocks)))
    ax2.set_yticklabels(stocks)
    ax2.set_xlabel('å¯ä¿¡åº¦åˆ†æ•¸')
    ax2.set_title('é æ¸¬å¯ä¿¡åº¦')
    ax2.set_xlim(0, 1)
    ax2.axvline(x=0.7, color='green', linestyle='--', alpha=0.5, label='é«˜å¯ä¿¡åº¦')
    ax2.axvline(x=0.4, color='orange', linestyle='--', alpha=0.5, label='ä¸­ç­‰å¯ä¿¡åº¦')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('prediction_confidence.png', dpi=300, bbox_inches='tight')
    plt.show()
```

---

## âš ï¸ æ³¨æ„äº‹é …

### 1. å¯ä¿¡åº¦ â‰  æº–ç¢ºåº¦

```
å¯ä¿¡åº¦é«˜ â‰  é æ¸¬ä¸€å®šæº–ç¢º
å¯ä¿¡åº¦ä½ â‰  é æ¸¬ä¸€å®šéŒ¯èª¤

å¯ä¿¡åº¦åªæ˜¯å‘Šè¨´ä½ ï¼š
- æ¨¡å‹å°é€™å€‹é æ¸¬æœ‰å¤šç¢ºå®š
- é€™å€‹é æ¸¬çš„ä¸ç¢ºå®šæ€§æœ‰å¤šå¤§
```

### 2. è³‡æ–™é‡å½±éŸ¿å¯ä¿¡åº¦

```
è³‡æ–™é‡å°‘ï¼ˆ31 ç­†ï¼‰:
- æ‰€æœ‰é æ¸¬çš„å¯ä¿¡åº¦éƒ½æœƒåä½
- é æ¸¬å€é–“æœƒå¾ˆå¯¬

è³‡æ–™é‡å¤šï¼ˆ1000+ ç­†ï¼‰:
- å¯ä¿¡åº¦æœƒæ›´å¯é 
- é æ¸¬å€é–“æœƒæ›´çª„
```

### 3. å¤–æ¨å•é¡Œ

```
å¦‚æœæ–°è³‡æ–™èˆ‡è¨“ç·´è³‡æ–™å·®ç•°å¾ˆå¤§ï¼š
- ç›¸ä¼¼åº¦åˆ†æ•¸æœƒå¾ˆä½
- å¯ä¿¡åº¦æœƒå¾ˆä½
- é æ¸¬å¯èƒ½ä¸å¯é 

ç¯„ä¾‹ï¼š
è¨“ç·´è³‡æ–™: ç”¢æ¥­ 10-20ï¼ˆç§‘æŠ€ã€å·¥æ¥­ï¼‰
æ–°è³‡æ–™: ç”¢æ¥­ 44ï¼ˆæ¶ˆè²»å“ï¼‰
â†’ ç›¸ä¼¼åº¦ä½ï¼Œå¯ä¿¡åº¦ä½
```

---

## ğŸ¯ ç¸½çµ

### å›ç­”ä½ çš„å•é¡Œ

**æ˜¯çš„ï¼Œé æ¸¬çµæœæœ‰ã€Œå¯ä¿¡åº¦ã€ï¼**

### ä¸»è¦æ–¹æ³•

1. **é æ¸¬å€é–“**ï¼šä¼°è¨ˆé æ¸¬å€¼çš„å¯èƒ½ç¯„åœ
2. **æ¨™æº–èª¤å·®**ï¼šé æ¸¬çš„å¹³å‡èª¤å·®
3. **æ¨¡å‹ä¸ç¢ºå®šæ€§**ï¼šä½¿ç”¨å¤šå€‹æ¨¡å‹æˆ– Bootstrap
4. **ç‰¹å¾µç›¸ä¼¼åº¦**ï¼šæ–°è³‡æ–™èˆ‡è¨“ç·´è³‡æ–™çš„ç›¸ä¼¼ç¨‹åº¦

### å¯¦ç”¨å»ºè­°

1. **ä½¿ç”¨ Bootstrap** ä¼°è¨ˆé æ¸¬å€é–“ï¼ˆæœ€å¯¦ç”¨ï¼‰
2. **è¨ˆç®—ç›¸ä¼¼åº¦** åˆ¤æ–·æ˜¯å¦å¤–æ¨
3. **ç¶œåˆå¤šå€‹æŒ‡æ¨™** å¾—å‡ºå¯ä¿¡åº¦åˆ†æ•¸
4. **æ ¹æ“šå¯ä¿¡åº¦æ±ºç­–**ï¼š
   - é«˜å¯ä¿¡åº¦ï¼ˆ> 0.7ï¼‰â†’ å¯ä»¥æ¡å–è¡Œå‹•
   - ä¸­ç­‰å¯ä¿¡åº¦ï¼ˆ0.4-0.7ï¼‰â†’ è¬¹æ…è©•ä¼°
   - ä½å¯ä¿¡åº¦ï¼ˆ< 0.4ï¼‰â†’ å»ºè­°è§€æœ›

### ä½ çš„å°ˆæ¡ˆ

ç”±æ–¼è³‡æ–™é‡å°‘ï¼ˆ31 ç­†ï¼‰ï¼Œç›®å‰æ‰€æœ‰é æ¸¬çš„å¯ä¿¡åº¦éƒ½æœƒåä½ã€‚å»ºè­°ï¼š
1. å…ˆæ”¶é›†æ›´å¤šè³‡æ–™
2. é”åˆ° 100+ ç­†å¾Œå†å¯¦ä½œå¯ä¿¡åº¦ä¼°è¨ˆ
3. ä½¿ç”¨ Bootstrap æ–¹æ³•æœ€ç°¡å–®æœ‰æ•ˆ

---

**è¨˜ä½**ï¼šå¯ä¿¡åº¦æ˜¯é¢¨éšªç®¡ç†çš„é‡è¦å·¥å…·ï¼Œä¸è¦åªçœ‹é æ¸¬å€¼ï¼Œä¹Ÿè¦çœ‹å¯ä¿¡åº¦ï¼
