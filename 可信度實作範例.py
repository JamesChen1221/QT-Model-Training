"""
é æ¸¬å¯ä¿¡åº¦å¯¦ä½œç¯„ä¾‹
ç°¡åŒ–ç‰ˆï¼šé©åˆå¿«é€Ÿæ•´åˆåˆ°ç¾æœ‰å°ˆæ¡ˆ
"""

import numpy as np
import pandas as pd
from sklearn.neighbors import NearestNeighbors
import xgboost as xgb


def predict_with_confidence_simple(model, X_new, X_train, y_train, n_bootstrap=30):
    """
    ç°¡åŒ–ç‰ˆï¼šä½¿ç”¨ Bootstrap ä¼°è¨ˆé æ¸¬å¯ä¿¡åº¦
    
    åƒæ•¸:
    - model: è¨“ç·´å¥½çš„æ¨¡å‹
    - X_new: æ–°è³‡æ–™ï¼ˆè¦é æ¸¬çš„ï¼‰
    - X_train: è¨“ç·´è³‡æ–™ç‰¹å¾µ
    - y_train: è¨“ç·´è³‡æ–™ç›®æ¨™
    - n_bootstrap: Bootstrap è¿­ä»£æ¬¡æ•¸ï¼ˆé è¨­ 30ï¼‰
    
    è¿”å›:
    - dict: åŒ…å«é æ¸¬å€¼ã€å¯ä¿¡åº¦ã€é æ¸¬å€é–“ç­‰è³‡è¨Š
    """
    
    # 1. åŸºæœ¬é æ¸¬
    prediction = model.predict(X_new)[0]
    
    # 2. Bootstrap ä¼°è¨ˆä¸ç¢ºå®šæ€§
    print(f"æ­£åœ¨è¨ˆç®—å¯ä¿¡åº¦ï¼ˆBootstrap {n_bootstrap} æ¬¡ï¼‰...")
    predictions = []
    
    for i in range(n_bootstrap):
        # é‡æ¡æ¨£è¨“ç·´è³‡æ–™
        indices = np.random.choice(len(X_train), len(X_train), replace=True)
        X_boot = X_train[indices]
        y_boot = y_train[indices]
        
        # è¨“ç·´æ¨¡å‹
        boot_model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.1,
            random_state=i,
            verbosity=0
        )
        boot_model.fit(X_boot, y_boot)
        
        # é æ¸¬
        pred = boot_model.predict(X_new)[0]
        predictions.append(pred)
    
    predictions = np.array(predictions)
    
    # 3. è¨ˆç®—çµ±è¨ˆé‡
    mean_pred = predictions.mean()
    std_pred = predictions.std()
    lower_95 = np.percentile(predictions, 2.5)
    upper_95 = np.percentile(predictions, 97.5)
    interval_width = upper_95 - lower_95
    
    # 4. è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆèˆ‡è¨“ç·´è³‡æ–™çš„ç›¸ä¼¼ç¨‹åº¦ï¼‰
    nn = NearestNeighbors(n_neighbors=min(5, len(X_train)))
    nn.fit(X_train)
    distances, _ = nn.kneighbors(X_new)
    avg_distance = distances.mean()
    
    # æ¨™æº–åŒ–è·é›¢ï¼ˆ0-1ï¼‰
    max_distance = np.linalg.norm(X_train.max(axis=0) - X_train.min(axis=0))
    similarity_score = max(0, 1 - (avg_distance / max_distance))
    
    # 5. è¨ˆç®—å¯ä¿¡åº¦åˆ†æ•¸ï¼ˆ0-1ï¼‰
    # å€é–“å¯¬åº¦åˆ†æ•¸ï¼ˆå€é–“è¶Šçª„è¶Šå¥½ï¼Œå‡è¨­ < 10% ç‚ºé«˜å¯ä¿¡åº¦ï¼‰
    interval_score = max(0, 1 - interval_width / 20)
    
    # æ¨¡å‹ä¸€è‡´æ€§åˆ†æ•¸ï¼ˆæ¨™æº–å·®è¶Šå°è¶Šå¥½ï¼Œå‡è¨­ < 3% ç‚ºé«˜å¯ä¿¡åº¦ï¼‰
    consistency_score = max(0, 1 - std_pred / 6)
    
    # ç¶œåˆå¯ä¿¡åº¦ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
    confidence = (
        0.4 * interval_score +      # 40% æ¬Šé‡ï¼šé æ¸¬å€é–“
        0.3 * similarity_score +     # 30% æ¬Šé‡ï¼šè³‡æ–™ç›¸ä¼¼åº¦
        0.3 * consistency_score      # 30% æ¬Šé‡ï¼šæ¨¡å‹ä¸€è‡´æ€§
    )
    
    # 6. å¯ä¿¡åº¦ç­‰ç´š
    if confidence > 0.7:
        confidence_level = "é«˜"
        recommendation = "å¯ä»¥æ¡å–è¡Œå‹•"
    elif confidence > 0.4:
        confidence_level = "ä¸­"
        recommendation = "è¬¹æ…è©•ä¼°"
    else:
        confidence_level = "ä½"
        recommendation = "å»ºè­°è§€æœ›"
    
    return {
        'prediction': prediction,
        'mean_prediction': mean_pred,
        'confidence_score': confidence,
        'confidence_level': confidence_level,
        'recommendation': recommendation,
        'interval_95': (lower_95, upper_95),
        'interval_width': interval_width,
        'std': std_pred,
        'similarity': similarity_score,
        'details': {
            'interval_score': interval_score,
            'consistency_score': consistency_score,
            'similarity_score': similarity_score
        }
    }


def print_confidence_result(result, stock_name=""):
    """ç¾åŒ–è¼¸å‡ºå¯ä¿¡åº¦çµæœ"""
    
    print("\n" + "="*60)
    if stock_name:
        print(f"è‚¡ç¥¨: {stock_name}")
    print("="*60)
    
    print(f"\nğŸ“Š é æ¸¬çµæœ:")
    print(f"  é æ¸¬å€¼: {result['prediction']:>8.2f}%")
    print(f"  å¹³å‡å€¼: {result['mean_prediction']:>8.2f}%")
    
    print(f"\nğŸ¯ å¯ä¿¡åº¦åˆ†æ:")
    print(f"  å¯ä¿¡åº¦åˆ†æ•¸: {result['confidence_score']:.2f} ({result['confidence_level']})")
    print(f"  å»ºè­°: {result['recommendation']}")
    
    print(f"\nğŸ“ˆ é æ¸¬å€é–“ (95%):")
    print(f"  ä¸‹ç•Œ: {result['interval_95'][0]:>8.2f}%")
    print(f"  ä¸Šç•Œ: {result['interval_95'][1]:>8.2f}%")
    print(f"  å¯¬åº¦: {result['interval_width']:>8.2f}%")
    
    print(f"\nğŸ” è©³ç´°æŒ‡æ¨™:")
    print(f"  é æ¸¬æ¨™æº–å·®: {result['std']:>8.2f}%")
    print(f"  è³‡æ–™ç›¸ä¼¼åº¦: {result['similarity']:>8.2f}")
    print(f"  å€é–“åˆ†æ•¸:   {result['details']['interval_score']:>8.2f}")
    print(f"  ä¸€è‡´æ€§åˆ†æ•¸: {result['details']['consistency_score']:>8.2f}")
    
    print("\n" + "="*60)


# ============================================================================
# ä½¿ç”¨ç¯„ä¾‹
# ============================================================================

if __name__ == "__main__":
    """
    ä½¿ç”¨ç¯„ä¾‹ï¼šæ•´åˆåˆ°ç¾æœ‰çš„é æ¸¬æµç¨‹
    """
    
    import joblib
    
    # 1. è¼‰å…¥æ¨¡å‹å’Œè³‡æ–™
    print("è¼‰å…¥æ¨¡å‹...")
    model_data = joblib.load('models/qt_model_é–‹ç›¤_pct.pkl')
    model = model_data['model']
    scaler = model_data['scaler']
    feature_columns = model_data['feature_columns']
    
    # 2. è¼‰å…¥è¨“ç·´è³‡æ–™ï¼ˆç”¨æ–¼è¨ˆç®—ç›¸ä¼¼åº¦ï¼‰
    print("è¼‰å…¥è¨“ç·´è³‡æ–™...")
    train_data = pd.read_excel('data/QT Training Data.xlsx', sheet_name='å·¥ä½œè¡¨1')
    
    # é è™•ç†è¨“ç·´è³‡æ–™ï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒçš„è™•ç†ï¼‰
    # ... é€™è£¡çœç•¥é è™•ç†æ­¥é©Ÿï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€è¦å®Œæ•´è™•ç† ...
    
    # å‡è¨­å·²ç¶“è™•ç†å¥½
    X_train = train_data[feature_columns].values
    y_train = train_data['#é–‹ç›¤ (%)'].values
    X_train_scaled = scaler.transform(X_train)
    
    # 3. è¼‰å…¥æ–°è³‡æ–™
    print("è¼‰å…¥é æ¸¬è³‡æ–™...")
    new_data = pd.read_excel('data/Stock TBP.xlsx', sheet_name='å·¥ä½œè¡¨1')
    
    # é è™•ç†æ–°è³‡æ–™
    # ... é€™è£¡çœç•¥é è™•ç†æ­¥é©Ÿ ...
    
    # å‡è¨­å·²ç¶“è™•ç†å¥½
    X_new = new_data[feature_columns].values
    X_new_scaled = scaler.transform(X_new)
    
    # 4. é æ¸¬ä¸¦è¨ˆç®—å¯ä¿¡åº¦
    print("\né–‹å§‹é æ¸¬...")
    result = predict_with_confidence_simple(
        model=model,
        X_new=X_new_scaled[:1],  # é æ¸¬ç¬¬ä¸€ç­†
        X_train=X_train_scaled,
        y_train=y_train,
        n_bootstrap=30  # å¯ä»¥èª¿æ•´ï¼Œè¶Šå¤šè¶Šæº–ç¢ºä½†è¶Šæ…¢
    )
    
    # 5. é¡¯ç¤ºçµæœ
    stock_name = new_data.iloc[0]['å…¬å¸ä»£ç¢¼'] if 'å…¬å¸ä»£ç¢¼' in new_data.columns else ""
    print_confidence_result(result, stock_name)
    
    # 6. æ ¹æ“šå¯ä¿¡åº¦æ±ºç­–
    print("\nğŸ’¡ æ±ºç­–å»ºè­°:")
    if result['confidence_score'] > 0.7:
        print("  âœ… é«˜å¯ä¿¡åº¦é æ¸¬")
        print("  â†’ å¯ä»¥æ ¹æ“šé æ¸¬å€¼æ¡å–è¡Œå‹•")
        print(f"  â†’ é æœŸé–‹ç›¤æ¼²å¹…: {result['prediction']:.2f}%")
    elif result['confidence_score'] > 0.4:
        print("  âš ï¸ ä¸­ç­‰å¯ä¿¡åº¦é æ¸¬")
        print("  â†’ å»ºè­°çµåˆå…¶ä»–åˆ†ææ–¹æ³•")
        print(f"  â†’ é æ¸¬ç¯„åœ: [{result['interval_95'][0]:.2f}%, {result['interval_95'][1]:.2f}%]")
    else:
        print("  âŒ ä½å¯ä¿¡åº¦é æ¸¬")
        print("  â†’ å»ºè­°è§€æœ›ï¼Œä¸è¦è¼•æ˜“è¡Œå‹•")
        print("  â†’ å¯èƒ½åŸå› ï¼š")
        if result['similarity'] < 0.5:
            print("     â€¢ æ–°è³‡æ–™èˆ‡è¨“ç·´è³‡æ–™å·®ç•°è¼ƒå¤§")
        if result['interval_width'] > 15:
            print("     â€¢ é æ¸¬å€é–“éå¯¬ï¼Œä¸ç¢ºå®šæ€§é«˜")
        if result['std'] > 5:
            print("     â€¢ æ¨¡å‹é æ¸¬ä¸ä¸€è‡´")
    
    print("\n" + "="*60)
    print("å®Œæˆï¼")
    print("="*60)
