# 特徵數量與資料量關係分析

## 🎯 核心問題

**當訓練資料非常多時，增加特徵數是否能讓模型更準確？**

**答案**：不一定！要看特徵的質量和資料量的比例。

---

## 📊 理論基礎

### 1. 資料量與特徵數的關係

#### 經驗法則（Rule of Thumb）

```
建議最少樣本數 = 10 × 特徵數量
更保守的建議 = 20-50 × 特徵數量
```

**範例**：
- 10 個特徵 → 至少需要 100-500 筆資料
- 50 個特徵 → 至少需要 500-2500 筆資料
- 100 個特徵 → 至少需要 1000-5000 筆資料

#### 你的專案現況

```
當前特徵數: 40 個（One-Hot 編碼）
當前資料量: 31 筆
建議資料量: 400-2000 筆

資料充足度: 31 / 400 = 7.75% ⚠️
```

### 2. 維度詛咒（Curse of Dimensionality）

**定義**：當特徵數量增加時，需要指數級增長的資料量來維持模型性能。

**問題**：
- 特徵空間變得稀疏
- 樣本之間的距離變大
- 模型難以找到有意義的模式
- 容易過擬合

**圖示**：
```
1D 空間（1個特徵）：
[====●====●====●====]  10個點就很密集

2D 空間（2個特徵）：
[  ●     ●        ]
[     ●      ●    ]  10個點開始稀疏

10D 空間（10個特徵）：
[                 ]
[  ●              ]  10個點非常稀疏
[                 ]
```

---

## ✅ 什麼時候增加特徵有幫助？

### 情況 1: 資料量充足 + 高質量特徵

**條件**：
- ✅ 資料量 > 20 × 特徵數
- ✅ 新特徵與目標有明確相關性
- ✅ 新特徵提供獨特資訊（不與現有特徵高度相關）

**範例**：
```python
# 當前: 40 特徵, 31 筆資料 → 不足
# 如果有: 40 特徵, 1000 筆資料 → 充足

# 新增有意義的特徵
新特徵 = [
    '成交量變化率',           # 與價格變化相關
    '相對強弱指數 (RSI)',      # 技術指標
    '布林通道位置',            # 價格位置
    '產業平均漲幅',            # 產業比較
    '市場整體趨勢'             # 大盤影響
]
```

**效果**：✅ 模型準確度提升

### 情況 2: 資料量充足 + 特徵選擇

**策略**：增加特徵後，使用特徵選擇方法

```python
from sklearn.feature_selection import SelectKBest, f_regression

# 1. 增加很多特徵（例如 100 個）
# 2. 使用特徵選擇保留最重要的（例如 30 個）

selector = SelectKBest(score_func=f_regression, k=30)
X_selected = selector.fit_transform(X, y)
```

**優點**：
- 讓模型自動找出最有用的特徵
- 避免手動判斷哪些特徵有用

### 情況 3: 使用正則化

**方法**：增加特徵，但使用 L1/L2 正則化

```python
# XGBoost 內建正則化
model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    reg_alpha=0.1,      # L1 正則化（Lasso）
    reg_lambda=1.0,     # L2 正則化（Ridge）
    random_state=42
)
```

**效果**：
- 自動降低不重要特徵的權重
- 防止過擬合

---

## ❌ 什麼時候增加特徵有害？

### 情況 1: 資料量不足

**問題**：
```
特徵數: 100 個
資料量: 50 筆
比例: 0.5 筆/特徵 ⚠️

結果: 嚴重過擬合
訓練集 R²: 1.0000 ✓
測試集 R²: -2.5000 ✗
```

**原因**：
- 模型記住了訓練資料的雜訊
- 無法泛化到新資料

### 情況 2: 低質量特徵（雜訊特徵）

**範例**：
```python
# 無意義的特徵
無用特徵 = [
    '公司代碼的 ASCII 值',     # 與股價無關
    '開盤日期的星期幾',         # 可能無關
    '隨機數',                  # 純雜訊
    '資料的行號'               # 完全無關
]
```

**效果**：❌ 模型準確度下降

### 情況 3: 高度相關的特徵（多重共線性）

**範例**：
```python
# 高度相關的特徵
相關特徵 = [
    '5日高價距離 (%)',
    '5日收盤價距離 (%)',      # 與上面高度相關
    '5日平均價距離 (%)',      # 與上面高度相關
]
```

**問題**：
- 特徵之間互相干擾
- 模型難以判斷哪個特徵重要
- 增加計算成本但沒有新資訊

---

## 📈 實際建議

### 你的專案現況（31 筆資料）

#### 當前狀態
```
特徵數: 40 個
資料量: 31 筆
比例: 0.775 筆/特徵 ⚠️
建議: 減少特徵或增加資料
```

#### 建議 1: 減少特徵（短期）

**方法**：使用特徵重要性分析

```python
# 查看特徵重要性
feature_importance = model.feature_importances_
important_features = feature_importance > 0.01  # 保留重要性 > 1% 的特徵

# 只保留重要特徵
X_reduced = X[:, important_features]
```

**目標**：將特徵數減少到 10-15 個

#### 建議 2: 增加資料（長期）

**目標資料量**：
- 最少：400 筆（10 × 40 特徵）
- 理想：1000+ 筆（25 × 40 特徵）

**達到後可以考慮**：
- ✅ 增加產業相關特徵
- ✅ 增加技術指標特徵
- ✅ 增加市場環境特徵

### 資料量達到 100+ 筆時

#### 可以增加的特徵類型

**1. 技術指標**
```python
新特徵 = [
    'RSI_14',              # 相對強弱指數
    'MACD',                # 移動平均收斂發散
    'BB_position',         # 布林通道位置
    'Volume_ratio',        # 成交量比率
    'ATR',                 # 平均真實範圍
]
```

**2. 產業比較**
```python
新特徵 = [
    '產業平均漲幅',
    '相對產業強度',
    '產業排名',
]
```

**3. 市場環境**
```python
新特徵 = [
    'SPY_change',          # 大盤變化
    'VIX',                 # 波動率指數
    'Market_sentiment',    # 市場情緒
]
```

**4. 時間特徵**
```python
新特徵 = [
    '是否財報季',
    '距離上次財報天數',
    '月份',
    '季度',
]
```

### 資料量達到 500+ 筆時

#### 可以使用更複雜的特徵

**1. 交互特徵**
```python
# 特徵之間的交互作用
X['盤前×消息情緒'] = X['盤前 (%)'] * X['消息情緒分數']
X['EPS×展望'] = X['EPS Surprise (%)'] * X['展望 (Guidance)']
```

**2. 多項式特徵**
```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)
```

**3. 深度學習特徵**
```python
# 使用神經網路自動提取特徵
# 需要更多資料（1000+ 筆）
```

---

## 🎓 關鍵原則

### 1. 質量 > 數量

```
10 個高質量特徵 > 100 個低質量特徵
```

**高質量特徵的特點**：
- ✅ 與目標變數有明確相關性
- ✅ 提供獨特資訊（不與其他特徵重複）
- ✅ 穩定可靠（不是雜訊）
- ✅ 有業務邏輯支持

### 2. 資料量是基礎

```
資料量不足時：
- 減少特徵 > 增加特徵
- 簡單模型 > 複雜模型

資料量充足時：
- 可以增加特徵
- 可以使用複雜模型
```

### 3. 特徵工程的黃金比例

```
理想比例 = 資料量 / 特徵數 ≥ 20

你的專案:
- 當前: 31 / 40 = 0.775 ⚠️
- 目標: 1000 / 40 = 25 ✓
```

### 4. 逐步增加，持續驗證

```python
# 不要一次增加太多特徵
# 逐步增加，每次驗證效果

步驟 1: 基線模型（10 個特徵）→ R² = 0.80
步驟 2: 增加 5 個特徵 → R² = 0.85 ✓ 有幫助
步驟 3: 再增加 5 個特徵 → R² = 0.83 ✗ 反而下降
結論: 保留步驟 2 的特徵組合
```

---

## 📊 實驗建議

### 當你有 100+ 筆資料時

**實驗 1: 特徵重要性分析**

```python
# 訓練模型
model.fit(X_train, y_train)

# 查看特徵重要性
importance_df = pd.DataFrame({
    '特徵': feature_names,
    '重要性': model.feature_importances_
}).sort_values('重要性', ascending=False)

print(importance_df.head(20))

# 只保留重要性 > 閾值的特徵
threshold = 0.01
important_features = importance_df[importance_df['重要性'] > threshold]['特徵'].tolist()
```

**實驗 2: 逐步增加特徵**

```python
# 從少到多測試
feature_sets = {
    '基礎': 10,
    '擴展': 20,
    '完整': 40,
    '增強': 60,
}

results = {}
for name, n_features in feature_sets.items():
    X_subset = X[:, :n_features]
    score = cross_val_score(model, X_subset, y, cv=5).mean()
    results[name] = score
    print(f"{name} ({n_features} 特徵): R² = {score:.4f}")
```

**實驗 3: 特徵選擇比較**

```python
from sklearn.feature_selection import SelectKBest, RFE

# 方法 1: 統計方法
selector1 = SelectKBest(k=20)
X_selected1 = selector1.fit_transform(X, y)

# 方法 2: 遞迴特徵消除
selector2 = RFE(model, n_features_to_select=20)
X_selected2 = selector2.fit_transform(X, y)

# 比較效果
```

---

## 🎯 總結

### 回答你的問題

**當訓練資料非常多時，增加特徵數是否能讓模型更準確？**

**答案**：
1. ✅ **可以**，如果：
   - 資料量 > 20 × 特徵數
   - 新特徵是高質量的
   - 新特徵提供獨特資訊

2. ❌ **不行**，如果：
   - 資料量不足
   - 新特徵是雜訊
   - 新特徵與現有特徵高度相關

3. 🎯 **最佳策略**：
   - 先確保資料量充足
   - 逐步增加高質量特徵
   - 持續驗證效果
   - 使用特徵選擇和正則化

### 你的專案建議

**當前（31 筆）**：
- 🔴 不要增加特徵
- ✅ 專注於收集更多資料
- ✅ 考慮減少特徵到 10-15 個

**達到 100 筆時**：
- 🟡 可以謹慎增加特徵
- ✅ 使用交叉驗證評估
- ✅ 保留重要特徵

**達到 500+ 筆時**：
- 🟢 可以大膽增加特徵
- ✅ 使用特徵選擇
- ✅ 嘗試複雜模型

---

**記住**：資料量是基礎，特徵質量是關鍵，數量不是越多越好！
